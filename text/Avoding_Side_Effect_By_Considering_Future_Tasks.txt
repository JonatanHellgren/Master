Reward function, specify what not to do and what to do.

A reward function stating what to do implicity states what to allow in order to acheive this. So side effect avoidence automatically says what to avoid doing.

Side effect problem Amodei et al. 2016, related to the frame problem in classical AI McCarthy and Hayes 1969. 
The frame problem - "How to specify the ways an action does not change the environment, and poses the challenge of specifing all possible non-effects"
Side effect problem - "Avoiding unnecessary changrs to the environment, and poses the same challenge of considering all the aspects of the envornment that the agent should not affect."

Usual way, define what not to do a priori. Can be tedious process and does not avoid side effects that where not foreseen or observed by the designer.

Reversibility as a auxiliary reward function. Problems with irreversible actions, omelette, also magnitude insensitive. Allows for alot of unneccesary actions aslong as the staring state is still reachable. Doesn't work since we want the environment to change, that is why we applied the agent.

More complex auxilary goals like future tasks avoids side effects more side effects, since they capture more about the dynamics in the environment, what could be done.

Interferance incentives arrises with auxilary goals, baseline is introduced to solve this.

Connection to reversibility, we can see reversibility as a auxilary goal where the agent recieves a reward of 1 for reaching it. This approach instead uses multiple diffect states and thus penalize all side effects that would have been penalized by revesibility reward. \autcite{}

"We say that an auxiliary reward r_{aux} induces an \textit{interferance incentive} iff the baseline policy is not optimal in the initial state for the auxiliary reward in the absence of the task reward."

"We show that for most future task distributions, the future task auxiliary reward induces an interference incentive: staying in x_0 has an higher no-reward value then following the baseline policy"

Agent only recieves auxiliary rewards if it performs atleast as well as the baseline policy with to auxiliary goals. If it reaches the goal then the first one also has to. 




