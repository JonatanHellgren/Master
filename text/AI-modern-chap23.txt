'In which we see how experiencing rewards and punishments can teach en agent how to maxmimze rewards in the future'

'Reinforcement learning differs from "just solving and MDP" because the agnet is not given the MDP as a problem to solve; the agent is in the MDP.'

Reward signal can be either sparse or intermediate. Intermediate makes the learning easier, but requires more modelling.

'learning to play Atari video games from raw visual input (Minih et al. 2013), controlling robots (Levine et al. 2016), and playing poker (Brown and Sandholm 2017)'

- Model-based reinforcement learning
  The agent models the whole environment with all transition probabilties and rewards.
  Learn a utility function U(s).

- Model-free reinforcement learning
  - Action-utility learning
    Q-learning, where the agent learns a Q-function Q(s,a), denoting the sum of rewards from state s onward if action a is taken. 
  - Policy search
    The agent learns apolicy pi(s) that maps directly from states to actions.
    
    
