
\documentclass{report}
\renewcommand{\chaptername}{}
\usepackage{titlesec}
\titleformat{\chapter}[hang] 
{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter:}{1em}{} 
% \documentclass[chapterprefix=false]{scrreprt}
% \makeatletter
% \renewcommand*{\chapterformat}{%
  % \mbox{\chapapp~\thechapter\autodot:\enskip}%
% }
% \makeatother

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
% \usepackage[backend=biber, style=numeric, sorting=none]{biblatex}
% \addbibresource{ref.bib}
\newcommand{\autocite}{}

\usepackage{float}

\usepackage[format=plain, labelfont={bf,it}, textfont=it]{caption}
\usepackage{subcaption}
% \usepackage{tikz}
\usepackage[top=3cm, bottom=3cm,inner=3cm, outer=3cm]{geometry}
\usepackage{fancyhdr}
% \usepackage{todonotes}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}

% \usepackage{lipsum}% http://ctan.org/pkg/lipsum


\theoremstyle{definition}
\newtheorem*{definition*}{Definition}
\newtheorem{definition}{Definition}[section]


% \usetikzlibrary{positioning, fit, arrows.meta, shapes, chains}
% \tikzset{Empty/.style={rectangle, fill=white, draw=white}}

\newcommand{\titel}{AI Risk} 
\newcommand{\undertitel}{Under development}


\newcommand{\examina}{\newline \noindent {\it Author}\\ 
  Jonatan Hellgren 
  \bigskip
}

\newcommand{\titelsidor}{
\pagestyle{fancy}
\fancyhf{}
\newgeometry{top=1.5cm, bottom=4cm,left=2 cm,right=1cm}
\fancyhead[L]{\includegraphics[width=155mm]{Chalmers_GU_svart-eps-converted-to.pdf}\\}
\addtolength{\voffset}{1cm}
\renewcommand{\headrulewidth}{1pt}

\renewcommand{\headrulewidth}{1pt}

\mbox{}
\vspace{50mm}

\noindent {\LARGE \titel}\bigskip\bigskip

\noindent {\large \undertitel}
\vspace{30mm}


\noindent {\large \examina}


\vfill
\hspace{-5.8 ex} \begin{tabular}[t]{lll}
Supervisor:& Olle Häggström\\
Examinator: & Torbjörn Lundh\end{tabular}
\renewcommand{\footrulewidth}{0.5pt}
\fancyfoot[L]{\vspace{0.1mm}\large Institutionen för Matematiska vetenskaper\\
%CHALMERS TEKNISKA HÖGSKOLA\\
%GÖTEBORGS UNIVERSITET\\
Gothenburg Sweden 2022}

 }

\begin{document}

\titelsidor
\newpage


\newgeometry{top=3cm, bottom=3cm,left=4 cm,right=4cm}

%\maketitle
% \newpage
\pagenumbering{roman}
% \pagestyle{plain}
\begin{abstract}
% \centering
ABSTRACT
\end{abstract}
\newpage
\pagestyle{plain}
\tableofcontents
\newpage
\pagenumbering{arabic}


%########################################################################%
% INTRODUCTION
%########################################################################%

\chapter{Introduction}

% argue that the subject is relevant
In recent human history we have seen a massive technological development, our lives today are severally different today compared to a century ago. Most of this development can be seen as the development of tools that we humans can make use of to carry on with increasing future development. In recent years artificial intelligence (AI) have been applied more commonly in the industry. This is currently due to it's outstanding ability to process massive amounts of data, which we are generating more then ever with the recent trend towards digitalization in our society. A key point here is that these systems still require humans to create and function them.

Many experts in the field of mathematics, computer science and even philosophy, believe that the future versions of AI will not require humans to function, they will be able to automate the human intelligence part as well. This is often referred to as artificial general intelligence (AGI). A significant difference with this shift is that it will increase the possible tasks that a single system can perform, in fact the amount of tasks possible would become arbitrary and they would be performed at human level of performance or higher. The effects of such a breakthrough could be on the same scale as the industrial revolution, but instead of automating physical labour we would instead have automated mental labour. 

% explain that there is a existential risk
Several AI researchers have raised warnings for future development of AI, Stuart Russel and Max Tegmark, Eliezer Yudkowsky to name but a few. The reason for this concern is that with such massive amounts of power they can have, it would be catastrophic if it where to be used in the wrong way. This would of course be a higher risk after a AGI breakthrough, since it could then also happen due to a slight misspecification of the system during it's creation, which would lead to them ending up unaligned with ours. The consequences could possibly be existential. 

In the upcoming century Toby Ord loosely estimates that the probability of an existential catastrophe is 17\%, out of which 10 percentage points are due to unaligned artificial intelligence \autocite{[precipice]}. Thus AI alignment is something worth spending resources on for the sake of humanity. 

% example of existential risk, conflicting goals
As for how such scenarios could play out a common example is the \textit{paperclip armageddon}. In which an paperclip maximizer is made super intelligent and starts accumulating resources such as hardware and money. Eventually the paperclip maximizer comes to a point where the existence of humans serves no purpose or possibly even having a negative effect on producing paperclips, and thus they become extinct.

% What we are doing today is not going to solve this issue


% say that the paper will be about dealing with how to solve this
The research field of creating safe AI has in the recent years literally exploded. We are a long way from solving the problem, most of what is being done today is mainly speculations and laying necessary foundations for future research. There are a lot of different subfields in this task and this report will specifically focus on the task of minimizing potential side effects on techniques we are applying today. Also with a purpose of spreading the ideas further, Nick Bostrom mentions in his book, \textit{Superintelligence} , that this problem is "\textit{... worthy of some of the next generation's best mathematical talent.}", and to attract those they need to at least know that they are needed. 
\autocite{Bostrom}





%########################################################################%
% BACKGROUND
%########################################################################%

\chapter{Background}

\section{Brief history of artificial intelligence}
% why intelligence is better  with machines, tranistors vs neurons
\begin{definition}[AI]
   Artificial intelligence (AI), is a computer program that is designed to solve a specific set of tasks. Usually these task require human level of intelligence. Since the set of task usually is limited it is also referred to as \textit{weak AI} or \textit{narrow AI}.
   \label{AI}
\end{definition}

The idea of creating AI, has been around since the dawn age of computer, where one of the founders of of modern computer science Alan Turing being the first one to define the concept. The field of AI research can be dated back to the 1956 Dartmouth summer research project on artificial intelligence, JOHN McCARTHY.... \\

The field has been studied ever since with some periods of breakthroughs and other with less innovation. 

Sybolic AI\\

Expert systems\\

Neural nets\\

\section{Emergence of artificial general intelligence}

As previously mentioned in Definition \ref{AI}, AI can be referred to as \textit{narrow AI}, this is due to the fact that if we would apply an AI on a task which it have not specifically been trained on the performance would most likely be horrible. Take for example DeepMinds AplhaGo that won against the worl champion Lee Sedol in the game of Go, if we where to apply the same system on the task of sorting mail, it would fail spectacularly. The reason is that a team of brilliant researchers at DeepMind designed the model specifically to be good at Go\footnote{In more recent years DeepMind have released a new AI called AlphaZero which has a more general approach and is thus able to play Go, Chess and Shogi. The set of task it however still limited.}. 

This takes us to our next definition.
\begin{definition}[AGI]
    Artificial generall intelligence (AGI), is an AI that can solve an arbitrary task with as good or better performance then a human is capable of, the main difference from AI being that the set of task is not bounded. AGI is also referred to as \textit{strong AI}.
\end{definition}
If an AGI would have been created we it should for example be able to play a game of Go, then drive it's car to it job where it sorts mail and much more. The implications of this would likely be something similar to the industrial revolution, but instead of automating physical labour, we would instead automate mental labour. 

As for predictions of when we are going to see the emergence of AGI there is a lot of uncertainty involved. WHEN EXPERTS GUESS. However with all the research and funding being focused on it, we are undoubtedly getting ever closer.

Since we have defined AGI to be able to solve an arbitrary amount of task, this would also include the creation of newer versions of it self. If it is better then the humans that created it at doing so and it keeps doing so recursively, an intelligence explosion would arise, also referred to as the \textit{singularity}.

\section{Potential risks and side effects}

Instrumental convergence, instumental goals, terminal goals\\

Orthogonality thesis\\

\section{Potential solutions}

\subsection{AI alignment}
content\\ 

Reward functions are easy to misspecify\\

example of unaligned; toxic GPT-3\\

define inner and outer alignment\\

potential consequences of unaligned AGI\\

\subsection{Impact measurements}

\section{Related work}



%########################################################################%
% THEORY
%########################################################################%

\chapter{Theoretical background}
To get a understanding of how intelligent agents(DEFINE AGENCY) will behave in the real world we need to make a few simplifications in order to make the problem feasible. The first one being that instead of modelling the real world we are instead going to make use of Markov decision processes. The second one being that we will have to use Reinforcement learning to achieve intelligent behaviour for agents. 

\section{Markov decision process}
A Markov decision process is a stochastic decision process, where the Markov property implies that the process is memoryless, meaning that the previous state do not have an effect on the next choice it only the current state that does. In mathematical terms it can be described as,
\[ p(a|s_t, s_{t-1}, s_{t-2}, ... , s_1) = p(a|s_t),\]
where $a$ is an action performed from state $s_t$ in time step $t$.

\begin{definition}[MDP]
    An Markov decision process (MDP), is defined as a tuple $(\mathcal{S}, \mathcal{A}, R, p, \gamma)$. $\mathcal{S}$ is the set of states, $\mathcal{A}$ is the set of actions, $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ the reward function, $p(s_{t+1}|s_t, a_t)$ is the transition probability from state $s_t$ to state $s_{t+1}$ given action $a_t$ at time step $t$, $\gamma$ is the discount factor typically defined in the range $\gamma \in [0, 1]$.
\end{definition}

The process it kept going until either a terminal state is reached or until a certain amount of time steps have been reached. A terminal state is a state where the process terminates, this can be some sort of goal and would thus yield a reward, but it could also yield no reward or negative reward.

The discount factor $\gamma$ has the important of describing how the agent values future rewards, with low values the agent favours more immediate rewards compared to future rewards, whereas for higher values the agent considers future rewards with more valuable. In environments with high uncertainty lower values of gamma might be more reasonable, since it might not be worth considering future rewards if they are not certain. The opposite holds for more deterministic environments where future rewards are of higher certainty, it might be a good idea to decrease the discount.


\subsection{Grid worlds}

\subsection{Solutions to Markov decision processes}

\section{Reinforcement learning}

\subsection{Q-learning}

\subsection{Deep Q-Learning}

\subsection{Inverse reinforcement learning}



%########################################################################%
% METHOD
%########################################################################%

\chapter{Methods}

\section{Simulations}



%########################################################################%
% RESULTS
%########################################################################%

\chapter{Results}



%########################################################################%
% DISCUSSION
%########################################################################%

\chapter{Discussion}



%########################################################################%
% CONCLUSION
%########################################################################%

\chapter{Conclusion}


% \printbibliography

\end{document}

